{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10lW3tJQKtQZTEtD4frAZ0X0xYGxKB4KG",
      "authorship_tag": "ABX9TyNMRZv7TSv2I7GAdTiODu7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satya-Mohan/Condition-control-statements/blob/main/AI_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "oPqukuKw5TPL"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file path\n",
        "base_path ='/content/drive/MyDrive/Satya_aclImdb'\n",
        "imdb_path_train='/train'\n",
        "imdb_path_test='/test'"
      ],
      "metadata": {
        "id": "l-mLMeA2A3kH"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_path = base_path+imdb_path_train\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "5ln5pC_LD4B-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tensorFlow dataset for the training testing and validation sets\n",
        "full_path = base_path+imdb_path_train\n",
        "df_imdb_train = text_dataset_from_directory(full_path,batch_size= batch_size, validation_split=0.2, subset='training', seed= 42 )\n",
        "df_imdb_val = text_dataset_from_directory(full_path,batch_size= batch_size, validation_split=0.2, subset='validation', seed= 42 )\n",
        "full_path = base_path+imdb_path_test\n",
        "df_imdb_test = text_dataset_from_directory(full_path,batch_size= batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sb2RrFqQD8sg",
        "outputId": "2911684c-e129-4949-e259-691c75b2d60c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25147 files belonging to 2 classes.\n",
            "Using 20118 files for training.\n",
            "Found 25147 files belonging to 2 classes.\n",
            "Using 5029 files for validation.\n",
            "Found 25070 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df_imdb_train))\n",
        "print(df_imdb_train.take(1))\n",
        "print(f\"Number of batches in df_imdb_train: {df_imdb_train.cardinality()}\")\n",
        "print(f\"Number of batches in df_imdb_val: {df_imdb_val.cardinality()}\")\n",
        "print(f\"Number of batches in df_imdb_test: {df_imdb_test.cardinality()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j2a06uFWEBT9",
        "outputId": "18e78645-0e08-4930-b5e3-9a628b256c26"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\n",
            "<_TakeDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
            "Number of batches in df_imdb_train: 629\n",
            "Number of batches in df_imdb_val: 158\n",
            "Number of batches in df_imdb_test: 784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in df_imdb_train.take(1):\n",
        "    for i in range(10):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zs8xuVdqEFSD",
        "outputId": "367d6ad8-9900-4745-a71b-4e816858f5fd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Released in 1965, but clearly shot years earlier, this is an inept little crime melodrama with some inept sexploitation up front. As usual for grindhouse flicks of era, there's a fair amount of undressing and dressing for no reason complemented by lousy music, annoying narration, and awkward editing. The coffee shop scene lays the excruciating groundwork, as we chop back and forth between characters to avoid actually seeing them speak their lines. All we get are reaction shots to the off-screen character's voice! 50s-pretty Misty Ayers strips to her French-cut panties a couple of times before the action gets started. She's accompanied continuously by what is apparently stock music from romantic to western to mother-does-the-dishes, mixed randomly to produce, among other things, the most thrilling cigarette lighting ever captured on film. Watch as he taps it! Watch as he strikes the match! Will he inhale or will he be captured by Apaches? Only time will tell!! The film tells the sordid tale of how Sally gets tricked into working in a whorehouse, falls for a dope, and can't escape. For some reason, we're treated to some of the most bored and boring hookers ever committed to film, literally doing their nails or knitting rather than entertaining the client\\xc3\\xa8le. Some stupendously lame comedy (boozy dame accidentally drinks milk! Har dee har!) and silent film acting doesn't help. This is one of the worst feature films I've ever seen, even on the Something Weird Video marquee. It's really more of a film curiosity for those interested in the history of cinema--very bad cinema.\"\n",
            "0\n",
            "b'\"Mad Dog Time\"...\"Trigger Happy\" whatever you wanna call it...simply doesn\\'t hit the mark. Maybe its just me, maybe i just don\\'t like Gangster comedies ( as i thought Oscar , Johney Dangerously and Mafia also sucked ) It\\'s probably more \"witty sharp wordplay\" than all out Comedy, only its not as witty and sharp as it ( or the other reviewers )Make it out to be. <br /><br />The Rick , Mick , Vic Thing was old to begin with making it a running gag was at times painful to watch. <br /><br />There wasn\\'t enough Changes of Location or Feel for the period they were supposed to be in. The Majority of the film was either set in \"Dreyfus\\'s Club\" or a variety of Offices /dim rooms... ( what was with that Sit down Gun stand off thing Goldblum kept winning ?) <br /><br />The supporting cast was... on Paper excellent ( great to see Silva & Drago)but characters were killed off before they had time to develop. and Richard Pryors cameo was a Joke ! The Romance and Love element of the film also bogged it down.<br /><br />4/10 I don\\'t think i\\'ll return to it anytime soon.'\n",
            "0\n",
            "b\"A really bad sequel. Part 1 had a lot of funny moments - part 2 is just bad (in a boring way) and obviously made to squeeze money out of the fans.<br /><br />Shame on you, Otto Waalkes!<br /><br />The only slightly amusing moment in the film is Helge Schneider who apparently seems to be pis*ed about the other characters. It's quite easy to identify with him...<br /><br />The screenplay is sloppy/non-existent. The director should do everyone a favor and quit his job immediately. The acting is worse than a 2nd grade school play. <br /><br />Technically the movie is awful as well, but who can blame the cinematographer/sound guys who had to work with such an untalented director?\"\n",
            "0\n",
            "b\"I was one of about 200 people that was lucky enough to see an early sneak of this film.<br /><br />Stardust follows Tristan a young man on a quest to find a fallen star and bring it back to the woman he loves in order to prove his love for her. The only catch is that the star has fallen on the other side of the wall, a doorway between England and a magical kingdom known as Stormhold.<br /><br />This film was just a joy to watch, it has something in it for everyone, all of the action scenes are played out beautifully and the comedy is spread out through the film making it funny without being corny. If I had to compare the likes to another film it would probably have to be The Princess Bride, a classic.<br /><br />All the performances are outstanding, the beautiful Claire Danes makes you love her in her portrayal of Yvaine the trusting naive star and under rated Michelle Pfeiffer delivers a stellar over the top performance as Larnia...but the performance to talk about is Robert De Niro...In every scene that he is in hands down he steals the show...<br /><br />If you are in the mood for a funny fantasy love story this is the film. Guys don't get turned off by the description there is enough action comedy and not to mention lots of eye candy with Claire Danes and Michelle Pfeiffer to keep you entertained throughout. The cinematography is dead on and keeps with the feel of the film...nothing about the film seems forced.\"\n",
            "1\n",
            "b\"This is a great movie from the lost age of reactionary made-for-television drama. My all-time favourite actor, Robert Culp skillfully plots a trajectory through uptight liberal fairmindedness and faith in the system, kneejerk conservativism and fear of crime, and homicidal psychosis. The teens are a collection of pure sneering evil stereotypes, and the eventual message of this film makes episodes of Dragnet look evenhanded by comparison. But what really shines in this is the great pace of the movie, building the fear and paranoia by degrees, as well as the feel of the whole California setting. The cars are really great as well, as I recall. I give this film a 10, and I defy anyone to watch this film and not enjoy every minute. Remember, just because it's made-for-television doesn't mean it isn't great art.\"\n",
            "1\n",
            "b'Oh yes, Sakura Killers is a goofy, horrible ninja movie, make no mistake. But it\\'s also an incredibly enjoyable one. This is largely thanks to the awesome presence of one Chuck Connors, who is billed as starring in the movie but really only shines in a few scenes. I suppose he\\'s supposed to be sort of an Obi Wan Kenobi type (\"The tough ninja-buster\", the box copy exclaims) but his \\'wisdom\\' is laughable. \"Move without thinking\"??? My friend says this is the sign of mental retardation, not of supreme concentration.<br /><br />But really, his two aides, Sonny and Dennis, have such horrible dialogue that \\'Brooklyn\\', as we call The Colonel, tends to shine in comparison. Especially watch for Dennis\\' logic regarding the \\'genetic splicing\\' the Sakura are involved with. If you know anything about cloning you will die laughing. And yes, this is a major plot point, folks.<br /><br />A terribly fun movie, Sakura Killers is a hard-to-find gem. I won\\'t spoil the \\'trick\\' ending for you either, except that it\\'s a perfect set up for a Sakura Killers 2. Too bad Chuck Connors died. :-( Because he does have a the smoothest ways of blowing away ninjas.'\n",
            "0\n",
            "b'There\\'s an inexhaustible hunger for \"basic training\" movies, so it\\'s surprising that this one got so little notice when first released. Looks likely to have a well-deserved second life on DVD/VHS.<br /><br />Tigerland isn\\'t uniformly great by any means, there are some terribly cliched characters (especially the portrayal of the NCO\\'s, makes you long for the return of Lee \\'Full Metal Jacket\\' Ermey) but the lead performance of Colin Farrell is the stuff of instant stardom. Charisma to burn and a role any actor would kill to get.'\n",
            "1\n",
            "b'Gruveyman2 (comment below)you are a complete idiot...blinded by ignorance by the very city you have allegiance to. Its that whiny arrogance, that you are ironically claiming the film exudes about SF, that makes you seem like such the typical LA A**hole! The only reason you felt the film was so self congratulatory about SF is because you are jealous. Of course you don\\'t know it because you are so LA jaded. First of all the film was completely factual about a beautiful city; what has been filmed there and what has been filmed by some of its more famous locals. It says nothing bad about LA; and these accomplished directors choose to live in a beautiful city over LA. They recognize that they went to film school in LA and are obviously proud of that fact. They recognize that SF is close to LA which is a benefit. The only negative thing that was said that relates to LA, was about the studio executives. The same studio executives that hated these guys movies when they first saw them, but then those same movies went on to be huge world-wide grossing films. So why wouldn\\'t they have animosity towards the studio executive establishment and studio system? These are the only people they are trying to \"disassociate from\" and for good reasons! Don\\'t be so sensitive! How can you say that Francis Ford Coppola is the \"so called\" San Francisco director? How is he not to be considered that? And who directed The Godfather? Coppola did. It was his vision that told the story on the screen that won it a best picture award. So what who gave him the job? He admits it in the documentary that he didn\\'t even want to do the movie....so what\\'s your point? And so what if Sophia wants to live in LA? And that proves your point how? And tell me how they are not truly independent when they are funding a lot of their own movies. Movies that are now considered classics. And, when they made movies from studio funding, one, it was LA that came to them and said we want you to make these pictures and two, they used the money that they made from doing these pictures to fund their own. They said exactly that in the film.<br /><br />\"Your bitchy and self congratulatory whining would take on an air of greater self respect and credence if you never set foot on the ground you so claim to be superior to in this film.\"<br /><br />How the hell can \"bitchy-ness\" and \"self-congratulation\" suddenly have an \"air\" of self respect and credence....if they never go to LA again? What a stupid and senseless comment! You inserted some big words in there....and just don\\'t know how to use them! And, by the way, they never claimed nor implied they were superior to LA! So what if they are giving a guy from New York an award in LA....again what the hell is your point? So if they go to LA or New York they are hypocrites by simply preferring to live in SF? You make no sense.<br /><br />San Francisco is proud of itself and its heritage and the people who make it what it is today. This film just focused on one aspect...film-making. For you to take the time and type up such nasty comments about the city (not the movie! But the city and its people) only proves what it is we Northern Californians hate about people from LA! THIS IS A GREAT DOCUMENTARY...VERY INTERESTING, ESPECIALLY IF YOU ARE FROM THE BAY AREA...BUT I RECOMMEND IT TO ANYONE.'\n",
            "1\n",
            "b'This is a very moving picture about 3 forty-something best friends in a small england town. One finds a passionate loves and a new beginning with a younger piano instructor, When tragedy strikes and hearts are changed forever. Definitely a film to have a box of tissues with you! A powerful piece of work. This is definitely one of my favorite films of all time.<br /><br />*SPOILER!!! SPOILER ALERT!! SPOILER!!*<br /><br />The main character is taken by her young, handsome piano instructor and a passionate romance blossoms. Her two jealous \"friends\" play an immature prank which quickly leads to tragedy. She loses her love and her friends in one foul swoop. In the end a unexpected surprise pulls them back together.(in my opinion her forgiveness is not warranted)'\n",
            "1\n",
            "b'When Philo Vance (Edmund Lowe) is standing precariously on the edge of a balcony high above the city, apparently hypnotized and just about to step to his death,it immediately reminded me of a nearly identical scene in another film made nine years later, \"The Woman in Green\" in which Sherlock Holmes (Basil Rathbone)is similarly about to hurl himself into space while being hypnotized. <br /><br />Happily, both Philo Vance and Sherlock Holmes survive these attempts at murder by unscrupulous criminals. Exciting cinematic suspense in both these scenes. When will they learn you can\\'t cloud the minds of great fictional detectives ?'\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standarize_data(data):\n",
        "    data_filtered = tf.strings.lower(data)\n",
        "    data_filtered = tf.strings.regex_replace(data_filtered, \"<br />\", \" \")\n",
        "    data_filtered = tf.strings.regex_replace(data_filtered, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "    return data_filtered"
      ],
      "metadata": {
        "id": "wBspd9FrEHfU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 15000\n",
        "\n",
        "unigram_vectorizer = TextVectorization(\n",
        "    standardize=custom_standarize_data,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='tf-idf',\n",
        "    ngrams=1,\n",
        "    #output_sequence_length=sequence_length)# can ony be used when output_mode = int\n",
        "    )"
      ],
      "metadata": {
        "id": "fdz3z2JrFLO9"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram model\n",
        "bigram_vectorizer = TextVectorization(\n",
        "    standardize=custom_standarize_data,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='tf-idf',\n",
        "    ngrams=2,\n",
        "    #output_sequence_length=sequence_length)\n",
        "    )\n",
        "\n",
        "text_ds = df_imdb_train.map(lambda x, y: x)\n",
        "unigram_vectorizer.adapt(text_ds)\n",
        "bigram_vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "Acd7jZjhJl0T"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bag-of-words model with unigrams\n",
        "unigram_model = tf.keras.models.Sequential([\n",
        "    unigram_vectorizer,\n",
        "    tf.keras.layers.Dense(52, activation=LeakyReLU(alpha=0.1)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "so6C_x7xJo2D"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit the bag-of-words model with unigrams\n",
        "unigram_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "unigram_model.fit(df_imdb_train, validation_data=df_imdb_val, epochs=10)\n",
        "\n",
        "loss, accuracy = unigram_model.evaluate(df_imdb_test)\n",
        "\n",
        "print('Unigram model performance:')\n",
        "print(\"Test set accuracy: {:.2f}\".format(accuracy))\n",
        "loss, accuracy = unigram_model.evaluate(df_imdb_test)\n",
        "print(\"Test set accuracy: {:.2f}\".format(accuracy))\n",
        "# Create a bag-of-words model with bigrams\n",
        "bigram_model = tf.keras.models.Sequential([\n",
        "    bigram_vectorizer,\n",
        "    tf.keras.layers.Dense(52, activation=LeakyReLU(alpha=0.1)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B-wL3j8UJ-Et",
        "outputId": "16229e41-a395-4eb6-c0e4-65acf9f8a96f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "629/629 [==============================] - 42s 64ms/step - loss: 0.3489 - accuracy: 0.8553 - val_loss: 0.2671 - val_accuracy: 0.8920\n",
            "Epoch 2/10\n",
            "629/629 [==============================] - 36s 57ms/step - loss: 0.1511 - accuracy: 0.9443 - val_loss: 0.3152 - val_accuracy: 0.8843\n",
            "Epoch 3/10\n",
            "629/629 [==============================] - 35s 56ms/step - loss: 0.0785 - accuracy: 0.9734 - val_loss: 0.3697 - val_accuracy: 0.8793\n",
            "Epoch 4/10\n",
            "629/629 [==============================] - 41s 65ms/step - loss: 0.0437 - accuracy: 0.9866 - val_loss: 0.4561 - val_accuracy: 0.8787\n",
            "Epoch 5/10\n",
            "629/629 [==============================] - 37s 58ms/step - loss: 0.0335 - accuracy: 0.9899 - val_loss: 0.4877 - val_accuracy: 0.8767\n",
            "Epoch 6/10\n",
            "629/629 [==============================] - 36s 57ms/step - loss: 0.0305 - accuracy: 0.9902 - val_loss: 0.5303 - val_accuracy: 0.8759\n",
            "Epoch 7/10\n",
            "629/629 [==============================] - 36s 57ms/step - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.6514 - val_accuracy: 0.8807\n",
            "Epoch 8/10\n",
            "629/629 [==============================] - 41s 65ms/step - loss: 0.0207 - accuracy: 0.9944 - val_loss: 0.6343 - val_accuracy: 0.8747\n",
            "Epoch 9/10\n",
            "629/629 [==============================] - 36s 57ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.6629 - val_accuracy: 0.8761\n",
            "Epoch 10/10\n",
            "629/629 [==============================] - 41s 65ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.7373 - val_accuracy: 0.8757\n",
            "784/784 [==============================] - 1890s 2s/step - loss: 0.9212 - accuracy: 0.8486\n",
            "Unigram model performance:\n",
            "Test set accuracy: 0.85\n",
            "784/784 [==============================] - 31s 40ms/step - loss: 0.9212 - accuracy: 0.8486\n",
            "Test set accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df_imdb_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TaC0y2RZQn1f",
        "outputId": "ed7dc1c9-b443-4a1e-8a59-f3ae60b90933"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "wcaK5KxuS4NW",
        "outputId": "bea22cda-e16f-45ec-e389-917dbb5075fb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-c3590f652ea1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sequential model with LSTM and embedding layer"
      ],
      "metadata": {
        "id": "3WveDWS4S68q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "fIQtNwYuS9EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running this line should output “Found 20000 files belonging to 2 classes”\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "\"path/train\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "\"path/test\", batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "MK91fhMNTQkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "max_tokens=max_tokens,\n",
        "output_mode=\"int\",\n",
        "output_sequence_length=max_length,\n",
        ")"
      ],
      "metadata": {
        "id": "x9_vQBq5TUCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_only_train_ds = train_ds.map(lambda x, y: x) # Prepare a dataset that only yields raw text inputs (no labels)"
      ],
      "metadata": {
        "id": "nmJg9j4ETgrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds) #Use that dataset to index the dataset vocabulary via the adapt() method"
      ],
      "metadata": {
        "id": "hIhy2UpzTiwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "n2tOVw9WTjLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") #This creates a Keras input layer that can accept sequences of integers with variable length (shape=(None,))"
      ],
      "metadata": {
        "id": "WgG8lfSETkja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256) \n",
        "#input_dim is the maximum number of tokens (words) in the vocabulary\n",
        "#output_dim is the size of the embedding vector - for each word in the vocabulary, a vector of size 256 is learned by the embedding layer"
      ],
      "metadata": {
        "id": "aB48olfGTp1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.LSTM(32)(embedded)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "AG1zT9fdTuao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "loss=\"binary_crossentropy\",\n",
        "metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "WkK_5oWhTxu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(int_train_ds, validation_data=int_test_ds, epochs=10)"
      ],
      "metadata": {
        "id": "RA0vhJzvT0XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "rxrDZWjOT2y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy training data\n",
        "texts = [\"positive text\", \"negative text\", \"neutral text\", \"positive review\", \"negative review\"]\n",
        "labels = [1, 0, 0, 1, 0]\n",
        "\n",
        "# Text vectorization layer\n",
        "max_tokens = 100\n",
        "text_vectorization = layers.TextVectorization(max_tokens=max_tokens)\n",
        "text_vectorization.adapt(texts)\n",
        "\n",
        "# Define the model\n",
        "embedding_dim = 16\n",
        "model = keras.Sequential([\n",
        "    text_vectorization,\n",
        "    layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim), #new thing\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 2\n",
        "epochs = 10\n",
        "history = model.fit(np.array(texts), np.array(labels), batch_size=batch_size, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "oZcrKCurT8VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Epoch 1/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 1.0917 - val_accuracy: 0.8642\n",
        "Epoch 2/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 1.0315 - val_accuracy: 0.8690\n",
        "Epoch 3/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 0.0128 - accuracy: 0.9970 - val_loss: 1.0379 - val_accuracy: 0.8714\n",
        "Epoch 4/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 1.1487 - val_accuracy: 0.8704\n",
        "Epoch 5/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 6.7784e-04 - accuracy: 0.9998 - val_loss: 1.2389 - val_accuracy: 0.8728\n",
        "Epoch 6/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 0.0214 - accuracy: 0.9962 - val_loss: 1.3027 - val_accuracy: 0.8524\n",
        "Epoch 7/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 0.0162 - accuracy: 0.9958 - val_loss: 0.8921 - val_accuracy: 0.8728\n",
        "Epoch 8/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 1.1458 - val_accuracy: 0.8706\n",
        "Epoch 9/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 3.8451e-04 - accuracy: 0.9999 - val_loss: 1.2083 - val_accuracy: 0.8746\n",
        "Epoch 10/10\n",
        "625/625 [==============================] - 3s 5ms/step - loss: 6.9276e-05 - accuracy: 1.0000 - val_loss: 1.2669 - val_accuracy: 0.8730\n",
        "Sequential model performance:\n",
        "782/782 [==============================] - 3s 3ms/step - loss: 1.3422 - accuracy: 0.8699"
      ],
      "metadata": {
        "id": "urtZVnpzT_AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VlucqlzRUJCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}